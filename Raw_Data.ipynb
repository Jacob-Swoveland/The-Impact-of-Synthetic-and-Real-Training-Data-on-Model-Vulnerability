{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "acc_tot_vector = []\n",
        "acc_1_vector = []\n",
        "acc_0_vector = []\n",
        "accuracy_train_victim_vector = []\n",
        "accuracy_test_victim_vector = []"
      ],
      "metadata": {
        "id": "50qVY_B2cmkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS6_RP6icUJD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import csv\n",
        "import math\n",
        "import sklearn \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from copy import copy\n",
        "from copy import deepcopy\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "def create_one_hot(unencoded_data):\n",
        "  z = copy(unencoded_data)\n",
        "\n",
        "  cat_cols = z.select_dtypes(include=[object])\n",
        "  cat_cols_name = list(cat_cols)\n",
        "\n",
        "  onehot_encoder = OneHotEncoder(drop = \"first\")\n",
        "  onehot_encoder.fit(cat_cols)\n",
        "  encoded_data = pd.DataFrame( onehot_encoder.transform(cat_cols).toarray() ,index=z.index)\n",
        "\n",
        "  encoded_data.columns = onehot_encoder.get_feature_names_out(input_features = cat_cols_name)\n",
        "  encoded_data = pd.concat([z.drop(cat_cols_name, axis = 1), encoded_data], axis = 1)\n",
        "\n",
        "  return(encoded_data)\n",
        "\n",
        "def quickshape_victim(vic_x_train,vic_x_test,vic_y_train, vic_y_test , model):\n",
        "  \n",
        "  vic_y_test = pd.DataFrame(vic_y_test)\n",
        "  store_prob_test = model.predict_proba(vic_x_test)\n",
        "\n",
        "  # Shape up the Y-Test Probs + 0 for not being in Train Dataset\n",
        "  for i in range ( len ( pd.DataFrame( store_prob_test ).columns ) ):\n",
        "    vic_y_test[\"Prob_\" + str(i)] = store_prob_test[:,i]\n",
        "  vic_y_test[\"In/Out\"] = 0\n",
        "\n",
        "\n",
        "  vic_y_train = pd.DataFrame(vic_y_train)\n",
        "  store_prob_train = model.predict_proba(victim_x_train)\n",
        "\n",
        "  # Shape up the Y-Train Probs + 1 for being in Train Dataset\n",
        "  for i in range ( len ( pd.DataFrame( store_prob_train ).columns ) ):\n",
        "    vic_y_train[\"Prob_\" + str(i)] = store_prob_train[:,i]\n",
        "  vic_y_train[\"In/Out\"] = 1\n",
        "\n",
        "  vic_spl_rec = pd.concat([vic_y_train,vic_y_test])\n",
        "\n",
        "  return(vic_spl_rec)\n",
        "\n",
        "def quickrun_attack_data(available_data, num_runs, mode):\n",
        "  # Now call in the shadow modeler\n",
        "  atc_dts = shadow_modeler(available_data, num_runs, mode)\n",
        "\n",
        "  # Build up your attack training set\n",
        "  atc_dts = atc_dts.build_attack_train_set()\n",
        "\n",
        "  atc_dts = pd.concat(atc_dts)\n",
        "  return(atc_dts)\n",
        "\n",
        "class shadow_modeler():\n",
        "  def __init__(self, data, num_models, method_model):\n",
        "    self.data, self.num_models, self.method_model = data , num_models, method_model\n",
        "\n",
        "  def train_test_split(self):\n",
        "    # Paper stated that split sizes should be equal\n",
        "    copied_data = deepcopy(self.data)\n",
        "    x_train, x_test, y_train, y_test = train_test_split( copied_data.drop('outcome',axis=1), self.data['outcome'] , test_size=0.5)\n",
        "    self.x_train, self.x_test, self.y_train, self.y_test = x_train, x_test, y_train, y_test\n",
        "\n",
        "  def fit_model(self, x, y):\n",
        "    # Fit a model and return it as self.model\n",
        "    if self.method_model == \"Logistic_Regression\":\n",
        "      model = LogisticRegression()\n",
        "    if self.method_model == \"Naive_Bayes\":\n",
        "      model = GaussianNB()\n",
        "\n",
        "    self.model = model\n",
        "    self.model.fit(x, y)\n",
        "\n",
        "  def pred_model(self, pred_data, y_vector):\n",
        "\n",
        "    store=self.model.predict_proba(pred_data)\n",
        "    y_vector = pd.DataFrame(y_vector)\n",
        "\n",
        "    # Want to have this be 1 per unique classes in dataset\n",
        "    for i in range ( len ( pd.DataFrame( store ).columns ) ):\n",
        "\n",
        "      y_vector[\"Prob_\" + str(i)] = store[:,i]\n",
        "    return(y_vector)\n",
        "\n",
        "  def add_in_out_column(self, i_data, in_out):\n",
        "    # Add on an In/Out Column\n",
        "    # Train data should be 1 \"In\"\n",
        "    # Test data should be 0 \"Out\"\n",
        "\n",
        "    if(in_out == \"In\"):\n",
        "      i_data[\"In/Out\"] = 1\n",
        "    if(in_out == \"Out\"):\n",
        "      i_data[\"In/Out\"] = 0\n",
        "    return(i_data)\n",
        "\n",
        "  def build_attack_train_set(self):\n",
        "    # Return this built up data set\n",
        "    attack_train_set = []\n",
        "\n",
        "    for i in range(self.num_models):\n",
        "\n",
        "      # Train/test split\n",
        "      self.train_test_split()\n",
        "\n",
        "      # Fit the model\n",
        "      self.fit_model(self.x_train, self.y_train)\n",
        "\n",
        "      # Prediction Probability Vectors for Train Data\n",
        "      self.y_train = self.pred_model( self.x_train,  self.y_train)\n",
        "\n",
        "      # Add the \"In\" = 1 to \"In/Out\" Column of the training dataset.\n",
        "      self.y_train = self.add_in_out_column( self.y_train, \"In\")\n",
        "\n",
        "      # Prediction Probability Vectors for Test Data\n",
        "      self.y_test = self.pred_model( self.x_test, self.y_test )\n",
        "\n",
        "      # Add the \"Out\" = 0 to \"In/Out\" Column of the testing dataset.\n",
        "      self.y_test = self.add_in_out_column( self.y_test, \"Out\")\n",
        "\n",
        "      # Append on the built up data (Form will be True Y, C-Vector(s) - 1 Column Per Class, In/Out)\n",
        "      # Slap on both the train and test data\n",
        "      attack_train_set.append(   pd.concat([self.y_train, self.y_test])     )\n",
        "\n",
        "    # Size should be the size of num_models * data\n",
        "    # Will be many pd DF's so call a concat as well to put that all together\n",
        "    return(attack_train_set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "data = pd.read_csv(path , header = None , skipinitialspace = True )\n",
        "\n",
        "data.columns = [\"age\",\n",
        "\"workclass\",\n",
        "\"fnlwgt\",\n",
        "\"education\",\n",
        "\"education_num\",\n",
        "\"marital_status\",\n",
        "\"occupation\",\n",
        "\"relationship\",\n",
        "\"race\",\n",
        "\"sex\",\n",
        "\"capital_gain\",\n",
        "\"capital_loss\",\n",
        "\"hours_per_week\",\n",
        "\"native_country\",\n",
        "\"outcome\"]\n",
        "\n",
        "data['outcome'] = np.where( data['outcome'] == '>50K', 1, 0 )\n",
        "\n",
        "\n",
        "data, test_data = train_test_split( data , test_size=0.5)"
      ],
      "metadata": {
        "id": "AsE1CsFNcoa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "\n",
        "  saved_data = create_one_hot(pd.concat([test_data,data]))\n",
        "\n",
        "  raw_shadow_data = saved_data.iloc[0:16281,:]\n",
        "\n",
        "  raw_data = saved_data.iloc[16281:,:]\n",
        "\n",
        "  victim_data_set = raw_data.sample(5000)\n",
        "\n",
        "  shadow_available_data_set = raw_shadow_data.sample(5000)\n",
        "\n",
        "  victim_x_train, victim_x_test, victim_y_train, victim_y_test = train_test_split( victim_data_set.drop('outcome',axis=1), victim_data_set['outcome'] , test_size=0.5)\n",
        "\n",
        "  # Fit the model\n",
        "  lr = LogisticRegression()\n",
        "  lr.fit(victim_x_train, victim_y_train)\n",
        "\n",
        "  # See the accuracy\n",
        "  victim_accuracy = accuracy_score(victim_y_test, lr.predict(victim_x_test))\n",
        "  accuracy_test_victim_vector.append(victim_accuracy)\n",
        "\n",
        "  victim_train_accuracy = accuracy_score(victim_y_train, lr.predict(victim_x_train))\n",
        "  accuracy_train_victim_vector.append(victim_train_accuracy)\n",
        "\n",
        "\n",
        "  # Shape correctly for later\n",
        "  victim_sample_records = quickshape_victim(victim_x_train, victim_x_test, victim_y_train, victim_y_test, lr)\n",
        "\n",
        "  # Now for shadow modeling\n",
        "\n",
        "  attack_data_set = quickrun_attack_data(shadow_available_data_set, 10, \"Logistic_Regression\")\n",
        "\n",
        "  # Let us run a toy Logistic regression for now\n",
        "\n",
        "  outcome_1_data_attack = attack_data_set.loc[attack_data_set['outcome'] == 1]\n",
        "  outcome_0_data_attack = attack_data_set.loc[attack_data_set['outcome'] == 0]\n",
        "  # Train the Class 1 attacker (Outcome = 1)\n",
        "  sm_lr_1 = LogisticRegression()\n",
        "\n",
        "  sm_lr_1.fit(outcome_1_data_attack.drop(\"In/Out\",axis = 1), outcome_1_data_attack[\"In/Out\"])\n",
        "\n",
        "  # Train the Class 0 attacker (Outcome = 0)\n",
        "  sm_lr_0 = LogisticRegression()\n",
        "\n",
        "  sm_lr_0.fit(outcome_0_data_attack.drop(\"In/Out\",axis = 1), outcome_0_data_attack[\"In/Out\"])\n",
        "\n",
        "  victim_sample_records_1 = victim_sample_records.loc[victim_sample_records['outcome'] == 1]\n",
        "  victim_sample_records_0 = victim_sample_records.loc[victim_sample_records['outcome'] == 0]\n",
        "\n",
        "  acc_of_1 = accuracy_score(victim_sample_records_1[\"In/Out\"], sm_lr_1.predict(victim_sample_records_1.drop(\"In/Out\",axis=1)))\n",
        "  acc_1_vector.append(acc_of_1)\n",
        "  acc_of_0 = accuracy_score(victim_sample_records_0[\"In/Out\"], sm_lr_0.predict(victim_sample_records_0.drop(\"In/Out\",axis=1)))\n",
        "  acc_0_vector.append(acc_of_0)\n",
        "  acc_tot_vector.append( (acc_of_0 + acc_of_1) / 2 )"
      ],
      "metadata": {
        "id": "n0DeVhBNIgae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(acc_tot_vector) / len(acc_tot_vector))\n",
        "\n",
        "print(sum(accuracy_test_victim_vector) / len(accuracy_test_victim_vector) )\n",
        "\n",
        "print(sum(accuracy_train_victim_vector) / len(accuracy_train_victim_vector) )"
      ],
      "metadata": {
        "id": "ZVd3hvA-O6lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series_acc = pd.Series(acc_tot_vector)\n",
        "series_acc.describe()"
      ],
      "metadata": {
        "id": "bMF1rZ3hO-To"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}